{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58fd630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import make_scorer, accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43713ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Functions ###\n",
    "\n",
    "def extract_s11_from_s1p(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Skip header lines that start with ! or #\n",
    "            if not line.startswith(('!', '#')):\n",
    "                columns = line.split()\n",
    "                # Collect only S11_Real (second column)\n",
    "                data.append(float(columns[1]))\n",
    "    return data\n",
    "\n",
    "# Function to parse Lg, Vds, and Vgs from filenames\n",
    "def parse_filename_parameters(filename):\n",
    "    # Initialize default values\n",
    "    Lg = Vds = Vgs = 'NA'\n",
    "    \n",
    "    # Use regex to search for Lg, Vds, and Vgs patterns in the filename\n",
    "    lg_match = re.search(r'Lg(\\d+)p(\\d+)', filename)\n",
    "    vds_match = re.search(r'Vds(\\d+)', filename)\n",
    "    vgs_match = re.search(r'Vgs(\\d+)', filename)\n",
    "\n",
    "    # Process the Lg match with \"p\" as decimal point\n",
    "    if lg_match:\n",
    "        Lg = float(f\"{lg_match.group(1)}.{lg_match.group(2)}\")\n",
    "    if vds_match:\n",
    "        Vds = int(vds_match.group(1))\n",
    "    if vgs_match:\n",
    "        Vgs = int(vgs_match.group(1))\n",
    "    \n",
    "    # If 'Opend' is in the filename, set Vds to 10000\n",
    "    if 'opend' in filename:\n",
    "        Vds = 10000\n",
    "    \n",
    "    return Lg, Vds, Vgs\n",
    "\n",
    "# Function to find repeating pattern\n",
    "def find_repeating_pattern(waveform, min_period=1000, num_periods=5):\n",
    "    # Calculate autocorrelation\n",
    "    autocorr = np.correlate(waveform, waveform, mode='full')\n",
    "    autocorr = autocorr[autocorr.size // 2:]  # Keep only the second half\n",
    "\n",
    "    # Find the first peak after lag=0\n",
    "    differences = np.diff(autocorr)  # Differences between consecutive points\n",
    "    peaks = np.where((differences[:-1] > 0) & (differences[1:] < 0))[0] + 1  # Peak detection\n",
    "    \n",
    "    # Find the period\n",
    "    if len(peaks) > 0:\n",
    "        period = peaks[0]  # The first peak indicates the repeating period\n",
    "        if period < min_period:\n",
    "            period = min_period  # Enforce minimum period\n",
    "    else:\n",
    "        period = min_period  # Default to minimum period if no peaks are found\n",
    "\n",
    "    # Extract multiple periods of the waveform\n",
    "    end_index = period * num_periods\n",
    "    repeating_pattern = waveform[:end_index]\n",
    "    \n",
    "    return repeating_pattern, period\n",
    "\n",
    "def filter_dataset_by_columns(main_dataset, subset_dataset, matching_columns):\n",
    "    \"\"\"\n",
    "    Filters rows in the main dataset where the values in matching columns \n",
    "    match those in the subset dataset.\n",
    "\n",
    "    Parameters:\n",
    "        main_dataset (pd.DataFrame): The primary dataset to filter.\n",
    "        subset_dataset (pd.DataFrame): The subset with matching criteria.\n",
    "        matching_columns (list): List of column names to match on.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A filtered dataset with matching rows.\n",
    "    \"\"\"\n",
    "    # Filter rows in the main dataset where matching column values are in the subset dataset\n",
    "    filtered_dataset = main_dataset[\n",
    "        main_dataset[matching_columns].apply(tuple, axis=1).isin(\n",
    "            subset_dataset[matching_columns].apply(tuple, axis=1)\n",
    "        )\n",
    "    ]\n",
    "    return filtered_dataset\n",
    "\n",
    "def smooth_dataframe_columns(df, group_size, fixed_start_cols, fixed_end_cols):\n",
    "    \n",
    "    if group_size <= 0:\n",
    "        raise ValueError(\"Group size must be greater than 0.\")\n",
    "    \n",
    "    # Separate fixed columns\n",
    "    fixed_start = df.iloc[:, :fixed_start_cols]\n",
    "    fixed_end = df.iloc[:, -fixed_end_cols:]\n",
    "    \n",
    "    # Columns to smooth (excluding fixed columns)\n",
    "    smooth_cols = df.iloc[:, fixed_start_cols:-fixed_end_cols]\n",
    "\n",
    "    # Smooth by averaging every `group_size` columns\n",
    "    smoothed_data = []\n",
    "    for i in range(0, smooth_cols.shape[1], group_size):\n",
    "        chunk = smooth_cols.iloc[:, i:i+group_size]\n",
    "        smoothed_data.append(chunk.mean(axis=1))\n",
    "\n",
    "    # Handle remaining columns if not a perfect multiple of group_size\n",
    "    if smooth_cols.shape[1] % group_size != 0:\n",
    "        remaining_cols = smooth_cols.iloc[:, -(smooth_cols.shape[1] % group_size):]\n",
    "        smoothed_data.append(remaining_cols.mean(axis=1))\n",
    "\n",
    "    # Combine fixed columns with smoothed data\n",
    "    smoothed_df = pd.concat([fixed_start] + smoothed_data + [fixed_end], axis=1)\n",
    "    return smoothed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfee2962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_sync_dataset(S1P_file_path, TDR_file_path):\n",
    "    \n",
    "    s11_data = {}\n",
    "    frequency_data = None\n",
    "    \n",
    "    # Loop over all .s1p files in the directory\n",
    "    for filename in os.listdir(S1P_file_path):\n",
    "        if filename.endswith('.s1p'):\n",
    "            file_path = os.path.join(S1P_file_path, filename)\n",
    "            s11_values = extract_s11_from_s1p(file_path)\n",
    "            \n",
    "            # Use the first file's frequency values as a reference\n",
    "            if frequency_data is None:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    frequency_data = [float(line.split()[0]) for line in file if not line.startswith(('!', '#'))]\n",
    "            \n",
    "            s11_data[filename] = s11_values\n",
    "            \n",
    "    s11_dataset = pd.DataFrame(s11_data).T\n",
    "    s11_dataset.columns = [f'Frequency_{i}' for i in range(len(s11_dataset.columns))]  # Set column names dynamically\n",
    "    s11_dataset.insert(0, 'File', s11_dataset.index) \n",
    "    \n",
    "    Duty_list = []\n",
    "\n",
    "    #for index, row in s11_dataframe.iterrows():\n",
    "    for i in s11_dataset['File']:\n",
    "        if 'dut1' in i:\n",
    "            Duty_list.append(1)\n",
    "        elif 'dut5' in i:\n",
    "            Duty_list.append(0)\n",
    "        else:\n",
    "            Duty_list.append('NA')\n",
    "    \n",
    "    s11_dataset = s11_dataset.drop(columns=['Frequency_0'], errors='ignore')  # Ignore error if column does not exist\n",
    "    \n",
    "    s11_dataset[['Lg', 'Vds', 'Vgs']] = s11_dataset['File'].apply(lambda x: pd.Series(parse_filename_parameters(x)))\n",
    "    s11_dataset = s11_dataset[['Lg', 'Vds', 'Vgs'] + [col for col in s11_dataset.columns if col.startswith('Frequency')]]\n",
    "    s11_dataset['Duty'] = Duty_list\n",
    "    \n",
    "    # Initialize a dictionary to store the loaded data\n",
    "    mat_data = {}\n",
    "\n",
    "    # Loop through the files in the folder\n",
    "    for file_name in os.listdir(TDR_file_path):\n",
    "        if file_name.endswith('.mat'):  # Check if the file is a .mat file\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(TDR_file_path, file_name)\n",
    "            # Load the .mat file and store it in the dictionary\n",
    "            mat_data[file_name] = loadmat(file_path)\n",
    "\n",
    "    # Access the loaded data as needed\n",
    "    tdr_train = mat_data.get('TDR_train.mat')\n",
    "    tdr_test = mat_data.get('TDR_test.mat')\n",
    "    tdr_val = mat_data.get('TDR_val.mat')\n",
    "    \n",
    "    # Extract and convert the datasets to DataFrames\n",
    "    train_data = pd.DataFrame(tdr_train[\"dataTDRtrain\"])\n",
    "    test_data = pd.DataFrame(tdr_test[\"dataTDRtest\"])\n",
    "    val_data = pd.DataFrame(tdr_val[\"dataTDRval\"])\n",
    "    \n",
    "    num_columns = train_data.shape[1]\n",
    "    column_names = [f\"t_{i+1}\" for i in range(num_columns)]\n",
    "\n",
    "    # Assign these column names to the DataFrame\n",
    "    train_data.columns = column_names\n",
    "    test_data.columns = column_names\n",
    "    val_data.columns = column_names\n",
    "    \n",
    "    # Specify the path to your Excel file\n",
    "    excel_file_path = r'D:\\Master_thesis\\creating_dataset\\TDR\\key_identifiers.xlsx'\n",
    "\n",
    "    # Load each sheet into a separate DataFrame\n",
    "    identifiers_train = pd.read_excel(excel_file_path, sheet_name='train', header=1)\n",
    "    identifiers_test = pd.read_excel(excel_file_path, sheet_name='test', header=1)\n",
    "    identifiers_val = pd.read_excel(excel_file_path, sheet_name='val', header=1)\n",
    "\n",
    "    train_dataset = pd.concat([identifiers_train, train_data], axis=1)\n",
    "    test_dataset = pd.concat([identifiers_test, test_data], axis=1)\n",
    "    val_dataset = pd.concat([identifiers_val, val_data], axis=1)\n",
    "\n",
    "    # The column to move to the last position (for example, column 'B')\n",
    "    col_to_move = 'Duty'\n",
    "\n",
    "    # Function to move a column to the last position in a DataFrame\n",
    "    def move_column_to_last(df, col_to_move):\n",
    "        cols = [col for col in df.columns if col != col_to_move]\n",
    "        df = df[cols + [col_to_move]]\n",
    "        return df\n",
    "\n",
    "    # Apply the function to each dataset\n",
    "    train_dataset = move_column_to_last(train_dataset, col_to_move)\n",
    "    test_dataset = move_column_to_last(test_dataset, col_to_move)\n",
    "    val_dataset = move_column_to_last(val_dataset, col_to_move)\n",
    "\n",
    "    TDR_dataset = pd.concat([train_dataset, test_dataset, val_dataset], axis=0, ignore_index=True)\n",
    "\n",
    "    TDR_dataset['Duty'] = TDR_dataset['Duty'].replace({1: 1, 5: 0})\n",
    "    print()\n",
    "    # Columns for comparison\n",
    "    common_columns = ['Lg', 'Vds', 'Vgs', 'Duty']\n",
    "\n",
    "    # Identify common samples\n",
    "    common_samples = pd.merge(s11_dataset[common_columns], TDR_dataset[common_columns], on=common_columns)\n",
    "\n",
    "    # Subset 1: Rows in df1 not in common samples\n",
    "    unsync_s11_dataset = s11_dataset[~s11_dataset[common_columns].apply(tuple, axis=1).isin(common_samples.apply(tuple, axis=1))]\n",
    "\n",
    "    # Subset 2: Common samples from df1 with all df1 columns\n",
    "    sync_s11_dataset = s11_dataset[s11_dataset[common_columns].apply(tuple, axis=1).isin(common_samples.apply(tuple, axis=1))]\n",
    "\n",
    "    # Subset 3: Common samples from df2 with all df2 columns\n",
    "    sync_TDR_dataset = TDR_dataset[TDR_dataset[common_columns].apply(tuple, axis=1).isin(common_samples.apply(tuple, axis=1))]\n",
    "\n",
    "    # Subset 4: Rows in df2 not in common samples\n",
    "    unsync_TDR_dataset = TDR_dataset[~TDR_dataset[common_columns].apply(tuple, axis=1).isin(common_samples.apply(tuple, axis=1))]\n",
    "\n",
    "    \n",
    "    return unsync_s11_dataset, sync_s11_dataset, sync_TDR_dataset, unsync_TDR_dataset, TDR_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e22b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "S1P_file_path = 'D:\\Master_thesis\\creating_dataset\\Dataset\\S11'\n",
    "TDR_file_path = r'D:\\Master_thesis\\creating_dataset\\TDR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0465844d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, _, sync_TDR_dataset, unsync_TDR_dataset, TDR_dataset= creating_sync_dataset(S1P_file_path, TDR_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34f8f929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 63005), (75, 63005))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sync_TDR_dataset.shape, TDR_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbfd1aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleModalityClassifier(nn.Module):\n",
    "    def __init__(self, seq_len, input_dim, num_classes, d_model=128, nhead=8, num_layers=1):\n",
    "        super(SingleModalityClassifier, self).__init__()\n",
    "      \n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.classifier = nn.Linear(d_model*seq_len, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.embedding2(x)  # Shape: [batch_size, seq_len2, d_model]\n",
    "\n",
    "        x = x.permute(1, 0, 2)  # Shape: [seq_len2, batch_size, d_model]\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute(1, 0, 2)  # Back to [batch_size, seq_len2, d_model]\n",
    "\n",
    "        # Flatten for classification\n",
    "        x = x.flatten(start_dim=1)  # Shape: [batch_size, (seq_len1 + seq_len2) * d_model]\n",
    "        out = self.classifier(x)  # Shape: [batch_size, num_classes]\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "075eb23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TDR_Classifier = SingleModalityClassifier(seq_len=1005, input_dim=1, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d596819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SingleModalityClassifier(\n",
       "  (embedding): Linear(in_features=1, out_features=128, bias=True)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=128640, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TDR_Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b674fbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_Scaling(Dataset, scaling_type='standard'):\n",
    "    \n",
    "    train_shape = int((Dataset.shape[0])*0.6)\n",
    "    test_shape = int((Dataset.shape[0])*0.2)\n",
    "    val_shape = int((Dataset.shape[0])*0.2)\n",
    "    print(train_shape, test_shape, val_shape)\n",
    "    \n",
    "    Dataset = Dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    Dataset_train, temp_data = train_test_split(Dataset, train_size=train_shape, random_state=None, shuffle=True)\n",
    "\n",
    "    # Step 2: Split temp_data into test and val\n",
    "    Dataset_test, Dataset_val = train_test_split(temp_data, train_size=test_shape, random_state=None, shuffle=True)\n",
    "    \n",
    "    Dataset_train = smooth_dataframe_columns(Dataset_train, 63, 3, 1)\n",
    "    Dataset_test = smooth_dataframe_columns(Dataset_test, 63, 3, 1)\n",
    "    Dataset_val = smooth_dataframe_columns(Dataset_val, 63, 3, 1)\n",
    "\n",
    "    X_Dataset_train = Dataset_train.iloc[:, 0:-1].values\n",
    "    y_Dataset_train = Dataset_train['Duty'].values\n",
    "\n",
    "    X_Dataset_test = Dataset_test.iloc[:, 0:-1].values\n",
    "    y_Dataset_test = Dataset_test['Duty'].values\n",
    "    print(y_Dataset_test)\n",
    "    X_Dataset_val = Dataset_val.iloc[:, 0:-1].values\n",
    "    y_Dataset_val = Dataset_val['Duty'].values\n",
    "    \n",
    "    # Mapping of scaling types to scaler objects\n",
    "    scalers = {\n",
    "        'standard': StandardScaler(),\n",
    "        'minmax': MinMaxScaler(),\n",
    "        'robust': RobustScaler(),\n",
    "        'maxabs': MaxAbsScaler()\n",
    "    }\n",
    "\n",
    "    # Check if the scaling_type is valid\n",
    "    if scaling_type not in scalers:\n",
    "        raise ValueError(f\"Invalid scaling_type. Choose from {list(scalers.keys())}.\")\n",
    "    \n",
    "    # Select the appropriate scaler\n",
    "    scaler = scalers[scaling_type]\n",
    "    \n",
    "    X_Dataset_train_scaled = scaler.fit_transform(X_Dataset_train)\n",
    "    X_Dataset_test_scaled = scaler.transform(X_Dataset_test)\n",
    "    X_Dataset_val_scaled = scaler.transform(X_Dataset_val)\n",
    "    \n",
    "    # Convert data to PyTorch tensors\n",
    "    X_Dataset_train_scaled = torch.tensor(X_Dataset_train_scaled, dtype=torch.float32)\n",
    "    y_Dataset_train = torch.tensor(y_Dataset_train, dtype=torch.long)\n",
    "    X_Dataset_test_scaled = torch.tensor(X_Dataset_test_scaled, dtype=torch.float32)\n",
    "    y_Dataset_test = torch.tensor(y_Dataset_test, dtype=torch.long)\n",
    "    X_Dataset_val_scaled = torch.tensor(X_Dataset_val_scaled, dtype=torch.float32)\n",
    "    y_Dataset_val = torch.tensor(y_Dataset_val, dtype=torch.long)\n",
    "\n",
    "    # Convert data and labels to TensorDatasets and create DataLoaders\n",
    "    Dataset_train_dataset = TensorDataset(X_Dataset_train_scaled, y_Dataset_train.long())\n",
    "    Dataset_val_dataset = TensorDataset(X_Dataset_val_scaled, y_Dataset_val.long())\n",
    "    Dataset_test_dataset = TensorDataset(X_Dataset_test_scaled, y_Dataset_test.long())\n",
    "\n",
    "    Dataset_train_loader = torch.utils.data.DataLoader(Dataset_train_dataset, batch_size=4, shuffle=True)\n",
    "    Dataset_test_loader = torch.utils.data.DataLoader(Dataset_test_dataset, batch_size=4)\n",
    "    Dataset_val_loader = torch.utils.data.DataLoader(Dataset_val_dataset, batch_size=4)\n",
    "    \n",
    "    return Dataset_train_loader, Dataset_test_loader, Dataset_val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87e6f09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 6 6\n",
      "[1 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "syncTDR_train_loader, syncTDR_test_loader, syncTDR_val_loader = data_Scaling(sync_TDR_dataset, scaling_type='standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6294985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3b37548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_TDR_model(model, train_loader, val_loader):\n",
    "    num_epochs=50 \n",
    "    patience=10\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(TDR_Classifier.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_targets, train_preds = [], []\n",
    "        \n",
    "        # Iterate over both loaders simultaneously (synchronized)\n",
    "        for train_data, train_labels in train_loader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(train_data.unsqueeze(-1))\n",
    "            loss = criterion(outputs, train_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            train_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            train_targets.extend(train_labels.cpu().numpy())\n",
    "                \n",
    "        train_accuracy = accuracy_score(train_targets, train_preds)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds, val_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for val_data, val_labels in val_loader:\n",
    "                \n",
    "                outputs = model(val_data.unsqueeze(-1))\n",
    "                loss = criterion(outputs, val_labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                val_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "                val_targets.extend(val_labels.cpu().numpy())\n",
    "\n",
    "        val_accuracy = accuracy_score(val_targets, val_preds)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs. Best Val Loss: {best_val_loss:.4f}\")\n",
    "            \n",
    "            break\n",
    "            \n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    test_preds, test_targets = [], []\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_data, test_labels in test_loader:\n",
    "\n",
    "            outputs = model(test_data.unsqueeze(-1))\n",
    "            test_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            test_targets.extend(test_labels.cpu().numpy())\n",
    "\n",
    "    test_accuracy = accuracy_score(test_targets, test_preds)\n",
    "    test_conf_matrix = confusion_matrix(test_targets, test_preds)\n",
    "    classi_report = classification_report(test_targets, test_preds)\n",
    "    print('Test accuracy:-', test_accuracy)\n",
    "    print(test_conf_matrix)\n",
    "    print('Confusion Matrix')\n",
    "    print(confusion_matrix)\n",
    "    print('Accuracy - ',np.round(test_accuracy,3))\n",
    "    print('Classification Report')\n",
    "    print(classi_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6215a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TDR_train_loader, TDR_test_loader, TDR_val_loader = data_Scaling(TDR_dataset, scaling_type='standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b71565ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chaitanya Mhetre\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Accuracy: 0.5556, Val Accuracy: 1.0000\n",
      "Epoch 2/50, Train Accuracy: 0.2778, Val Accuracy: 1.0000\n",
      "Epoch 3/50, Train Accuracy: 0.5000, Val Accuracy: 0.6667\n",
      "Epoch 4/50, Train Accuracy: 0.7222, Val Accuracy: 0.6667\n",
      "Epoch 5/50, Train Accuracy: 0.8333, Val Accuracy: 0.8333\n",
      "Epoch 6/50, Train Accuracy: 0.9444, Val Accuracy: 0.3333\n",
      "Epoch 7/50, Train Accuracy: 0.8889, Val Accuracy: 0.3333\n",
      "Epoch 8/50, Train Accuracy: 0.9444, Val Accuracy: 0.6667\n",
      "Epoch 9/50, Train Accuracy: 0.8333, Val Accuracy: 0.5000\n",
      "Epoch 10/50, Train Accuracy: 1.0000, Val Accuracy: 0.3333\n",
      "Epoch 11/50, Train Accuracy: 1.0000, Val Accuracy: 0.3333\n",
      "Early stopping triggered after 11 epochs. Best Val Loss: 0.0000\n",
      "Test accuracy:- 0.5\n",
      "[[3 0]\n",
      " [3 0]]\n",
      "Confusion Matrix\n",
      "<function confusion_matrix at 0x000002B37FCA1D30>\n",
      "Accuracy -  0.5\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         3\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.50         6\n",
      "   macro avg       0.25      0.50      0.33         6\n",
      "weighted avg       0.25      0.50      0.33         6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chaitanya Mhetre\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Chaitanya Mhetre\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Chaitanya Mhetre\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "TDR_Classifier = SingleModalityClassifier(seq_len2=1005, input_dim=1, num_classes=2)\n",
    "\n",
    "train_TDR_model(TDR_Classifier, syncTDR_train_loader, syncTDR_val_loader)\n",
    "test_model(TDR_Classifier, syncTDR_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9755ee66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
