{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60c34e1d-37d2-4855-9997-be6be30cbc4e",
   "metadata": {},
   "source": [
    "## Transformer Multimodal (30 Synchronised Samples) - Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "646a5918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import make_scorer, accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8623e9",
   "metadata": {},
   "source": [
    "## Extracting data from .s1p files and .mat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a2600c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Functions ###\n",
    "\n",
    "def extract_s11_from_s1p(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Skip header lines that start with ! or #\n",
    "            if not line.startswith(('!', '#')):\n",
    "                columns = line.split()\n",
    "                # Collect only S11_Real (second column)\n",
    "                data.append(float(columns[1]))\n",
    "    return data\n",
    "\n",
    "# Function to parse Lg, Vds, and Vgs from filenames\n",
    "def parse_filename_parameters(filename):\n",
    "    # Initialize default values\n",
    "    Lg = Vds = Vgs = 'NA'\n",
    "    \n",
    "    # Use regex to search for Lg, Vds, and Vgs patterns in the filename\n",
    "    lg_match = re.search(r'Lg(\\d+)p(\\d+)', filename)\n",
    "    vds_match = re.search(r'Vds(\\d+)', filename)\n",
    "    vgs_match = re.search(r'Vgs(\\d+)', filename)\n",
    "\n",
    "    # Process the Lg match with \"p\" as decimal point\n",
    "    if lg_match:\n",
    "        Lg = float(f\"{lg_match.group(1)}.{lg_match.group(2)}\")\n",
    "    if vds_match:\n",
    "        Vds = int(vds_match.group(1))\n",
    "    if vgs_match:\n",
    "        Vgs = int(vgs_match.group(1))\n",
    "    \n",
    "    # If 'Opend' is in the filename, set Vds to 10000\n",
    "    if 'opend' in filename:\n",
    "        Vds = 10000\n",
    "    \n",
    "    return Lg, Vds, Vgs\n",
    "\n",
    "# Function to find repeating pattern\n",
    "def find_repeating_pattern(waveform, min_period=1000, num_periods=5):\n",
    "    # Calculate autocorrelation\n",
    "    autocorr = np.correlate(waveform, waveform, mode='full')\n",
    "    autocorr = autocorr[autocorr.size // 2:]  # Keep only the second half\n",
    "\n",
    "    # Find the first peak after lag=0\n",
    "    differences = np.diff(autocorr)  # Differences between consecutive points\n",
    "    peaks = np.where((differences[:-1] > 0) & (differences[1:] < 0))[0] + 1  # Peak detection\n",
    "    \n",
    "    # Find the period\n",
    "    if len(peaks) > 0:\n",
    "        period = peaks[0]  # The first peak indicates the repeating period\n",
    "        if period < min_period:\n",
    "            period = min_period  # Enforce minimum period\n",
    "    else:\n",
    "        period = min_period  # Default to minimum period if no peaks are found\n",
    "\n",
    "    # Extract multiple periods of the waveform\n",
    "    end_index = period * num_periods\n",
    "    repeating_pattern = waveform[:end_index]\n",
    "    \n",
    "    return repeating_pattern, period\n",
    "\n",
    "def filter_dataset_by_columns(main_dataset, subset_dataset, matching_columns):\n",
    "    \"\"\"\n",
    "    Filters rows in the main dataset where the values in matching columns \n",
    "    match those in the subset dataset.\n",
    "\n",
    "    Parameters:\n",
    "        main_dataset (pd.DataFrame): The primary dataset to filter.\n",
    "        subset_dataset (pd.DataFrame): The subset with matching criteria.\n",
    "        matching_columns (list): List of column names to match on.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A filtered dataset with matching rows.\n",
    "    \"\"\"\n",
    "    # Filter rows in the main dataset where matching column values are in the subset dataset\n",
    "    filtered_dataset = main_dataset[\n",
    "        main_dataset[matching_columns].apply(tuple, axis=1).isin(\n",
    "            subset_dataset[matching_columns].apply(tuple, axis=1)\n",
    "        )\n",
    "    ]\n",
    "    return filtered_dataset\n",
    "\n",
    "def smooth_dataframe_columns(df, group_size, fixed_start_cols, fixed_end_cols):\n",
    "    \n",
    "    if group_size <= 0:\n",
    "        raise ValueError(\"Group size must be greater than 0.\")\n",
    "    \n",
    "    # Separate fixed columns\n",
    "    fixed_start = df.iloc[:, :fixed_start_cols]\n",
    "    fixed_end = df.iloc[:, -fixed_end_cols:]\n",
    "    \n",
    "    # Columns to smooth (excluding fixed columns)\n",
    "    smooth_cols = df.iloc[:, fixed_start_cols:-fixed_end_cols]\n",
    "\n",
    "    # Smooth by averaging every `group_size` columns\n",
    "    smoothed_data = []\n",
    "    for i in range(0, smooth_cols.shape[1], group_size):\n",
    "        chunk = smooth_cols.iloc[:, i:i+group_size]\n",
    "        smoothed_data.append(chunk.mean(axis=1))\n",
    "\n",
    "    # Handle remaining columns if not a perfect multiple of group_size\n",
    "    if smooth_cols.shape[1] % group_size != 0:\n",
    "        remaining_cols = smooth_cols.iloc[:, -(smooth_cols.shape[1] % group_size):]\n",
    "        smoothed_data.append(remaining_cols.mean(axis=1))\n",
    "\n",
    "    # Combine fixed columns with smoothed data\n",
    "    smoothed_df = pd.concat([fixed_start] + smoothed_data + [fixed_end], axis=1)\n",
    "    return smoothed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92369409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_sync_dataset(S1P_file_path, TDR_file_path):\n",
    "    \n",
    "    s11_data = {}\n",
    "    frequency_data = None\n",
    "    \n",
    "    # Loop over all .s1p files in the directory\n",
    "    for filename in os.listdir(S1P_file_path):\n",
    "        if filename.endswith('.s1p'):\n",
    "            file_path = os.path.join(S1P_file_path, filename)\n",
    "            s11_values = extract_s11_from_s1p(file_path)\n",
    "            \n",
    "            # Use the first file's frequency values as a reference\n",
    "            if frequency_data is None:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    frequency_data = [float(line.split()[0]) for line in file if not line.startswith(('!', '#'))]\n",
    "            \n",
    "            s11_data[filename] = s11_values\n",
    "            \n",
    "    s11_dataset = pd.DataFrame(s11_data).T\n",
    "    s11_dataset.columns = [f'Frequency_{i}' for i in range(len(s11_dataset.columns))]  # Set column names dynamically\n",
    "    s11_dataset.insert(0, 'File', s11_dataset.index) \n",
    "    \n",
    "    Duty_list = []\n",
    "\n",
    "    #for index, row in s11_dataframe.iterrows():\n",
    "    for i in s11_dataset['File']:\n",
    "        if 'dut1' in i:\n",
    "            Duty_list.append(1)\n",
    "        elif 'dut5' in i:\n",
    "            Duty_list.append(0)\n",
    "        else:\n",
    "            Duty_list.append('NA')\n",
    "    \n",
    "    s11_dataset = s11_dataset.drop(columns=['Frequency_0'], errors='ignore')  # Ignore error if column does not exist\n",
    "    \n",
    "    s11_dataset[['Lg', 'Vds', 'Vgs']] = s11_dataset['File'].apply(lambda x: pd.Series(parse_filename_parameters(x)))\n",
    "    s11_dataset = s11_dataset[['Lg', 'Vds', 'Vgs'] + [col for col in s11_dataset.columns if col.startswith('Frequency')]]\n",
    "    s11_dataset['Duty'] = Duty_list\n",
    "    \n",
    "    # Initialize a dictionary to store the loaded data\n",
    "    mat_data = {}\n",
    "\n",
    "    # Loop through the files in the folder\n",
    "    for file_name in os.listdir(TDR_file_path):\n",
    "        if file_name.endswith('.mat'):  # Check if the file is a .mat file\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(TDR_file_path, file_name)\n",
    "            # Load the .mat file and store it in the dictionary\n",
    "            mat_data[file_name] = loadmat(file_path)\n",
    "\n",
    "    # Access the loaded data as needed\n",
    "    tdr_train = mat_data.get('TDR_train.mat')\n",
    "    tdr_test = mat_data.get('TDR_test.mat')\n",
    "    tdr_val = mat_data.get('TDR_val.mat')\n",
    "    \n",
    "    # Extract and convert the datasets to DataFrames\n",
    "    train_data = pd.DataFrame(tdr_train[\"dataTDRtrain\"])\n",
    "    test_data = pd.DataFrame(tdr_test[\"dataTDRtest\"])\n",
    "    val_data = pd.DataFrame(tdr_val[\"dataTDRval\"])\n",
    "    \n",
    "    num_columns = train_data.shape[1]\n",
    "    column_names = [f\"t_{i+1}\" for i in range(num_columns)]\n",
    "\n",
    "    # Assign these column names to the DataFrame\n",
    "    train_data.columns = column_names\n",
    "    test_data.columns = column_names\n",
    "    val_data.columns = column_names\n",
    "    \n",
    "    # Specify the path to your Excel file\n",
    "    excel_file_path = r'C:\\Master_thesis\\creating_dataset\\TDR\\key_identifiers.xlsx'\n",
    "\n",
    "    # Load each sheet into a separate DataFrame\n",
    "    identifiers_train = pd.read_excel(excel_file_path, sheet_name='train', header=1)\n",
    "    identifiers_test = pd.read_excel(excel_file_path, sheet_name='test', header=1)\n",
    "    identifiers_val = pd.read_excel(excel_file_path, sheet_name='val', header=1)\n",
    "\n",
    "    train_dataset = pd.concat([identifiers_train, train_data], axis=1)\n",
    "    test_dataset = pd.concat([identifiers_test, test_data], axis=1)\n",
    "    val_dataset = pd.concat([identifiers_val, val_data], axis=1)\n",
    "\n",
    "    # The column to move to the last position (for example, column 'B')\n",
    "    col_to_move = 'Duty'\n",
    "\n",
    "    # Function to move a column to the last position in a DataFrame\n",
    "    def move_column_to_last(df, col_to_move):\n",
    "        cols = [col for col in df.columns if col != col_to_move]\n",
    "        df = df[cols + [col_to_move]]\n",
    "        return df\n",
    "\n",
    "    # Apply the function to each dataset\n",
    "    train_dataset = move_column_to_last(train_dataset, col_to_move)\n",
    "    test_dataset = move_column_to_last(test_dataset, col_to_move)\n",
    "    val_dataset = move_column_to_last(val_dataset, col_to_move)\n",
    "\n",
    "    TDR_dataset = pd.concat([train_dataset, test_dataset, val_dataset], axis=0, ignore_index=True)\n",
    "\n",
    "    TDR_dataset['Duty'] = TDR_dataset['Duty'].replace({1: 1, 5: 0})\n",
    "    print()\n",
    "    # Columns for comparison\n",
    "    common_columns = ['Lg', 'Vds', 'Vgs', 'Duty']\n",
    "\n",
    "    # Identify common samples\n",
    "    common_samples = pd.merge(s11_dataset[common_columns], TDR_dataset[common_columns], on=common_columns)\n",
    "\n",
    "    # Subset 1: Rows in df1 not in common samples\n",
    "    unsync_s11_dataset = s11_dataset[~s11_dataset[common_columns].apply(tuple, axis=1).isin(common_samples.apply(tuple, axis=1))]\n",
    "\n",
    "    # Subset 2: Common samples from df1 with all df1 columns\n",
    "    sync_s11_dataset = s11_dataset[s11_dataset[common_columns].apply(tuple, axis=1).isin(common_samples.apply(tuple, axis=1))]\n",
    "\n",
    "    # Subset 3: Common samples from df2 with all df2 columns\n",
    "    sync_TDR_dataset = TDR_dataset[TDR_dataset[common_columns].apply(tuple, axis=1).isin(common_samples.apply(tuple, axis=1))]\n",
    "\n",
    "    # Subset 4: Rows in df2 not in common samples\n",
    "    unsync_TDR_dataset = TDR_dataset[~TDR_dataset[common_columns].apply(tuple, axis=1).isin(common_samples.apply(tuple, axis=1))]\n",
    "\n",
    "    \n",
    "    return unsync_s11_dataset, sync_s11_dataset, sync_TDR_dataset, unsync_TDR_dataset, TDR_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "334c156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "S1P_file_path = r'C:\\Master_thesis\\creating_dataset\\Dataset\\S11'\n",
    "TDR_file_path = r'C:\\Master_thesis\\creating_dataset\\TDR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "810e84fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, sync_s11_dataset, sync_TDR_dataset,unsync_TDR_dataset, TDR_dataset= creating_sync_dataset(S1P_file_path, TDR_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa6f324c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 1004), (30, 63005))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sync_s11_dataset.shape,  sync_TDR_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab3a27a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_Scaling_Sync(sync_s11_dataset, sync_TDR_dataset, scaling_type='standard'):\n",
    "    \n",
    "    train_shape = int((sync_s11_dataset.shape[0])*0.6)\n",
    "    test_shape = int((sync_s11_dataset.shape[0])*0.2)\n",
    "    val_shape = int((sync_s11_dataset.shape[0])*0.2)\n",
    "\n",
    "    sync_s11_dataset = sync_s11_dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Step 1: Split into train and temp_data (test + val)\n",
    "    s11_train, temp_data = train_test_split(sync_s11_dataset, train_size=train_shape, random_state=42, stratify=sync_s11_dataset[\"Duty\"])\n",
    "\n",
    "    # Step 2: Split temp_data into test and val (ensuring balance in \"Duty\")\n",
    "    s11_test, s11_val = train_test_split(temp_data, train_size=test_shape, random_state=42, stratify=temp_data[\"Duty\"])\n",
    "    \n",
    "    # Columns for matching\n",
    "    matching_column = ['Lg', 'Vds', 'Vgs', 'Duty']\n",
    "    \n",
    "    TDR_train = filter_dataset_by_columns(sync_TDR_dataset, s11_train, matching_column)\n",
    "    TDR_test = filter_dataset_by_columns(sync_TDR_dataset, s11_test, matching_column)\n",
    "    TDR_val = filter_dataset_by_columns(sync_TDR_dataset, s11_val, matching_column)\n",
    "    \n",
    "    # Perform an inner merge to keep only rows where both A and B match\n",
    "    merged_train = pd.merge(s11_train, TDR_train, on=['Lg', 'Vds', 'Vgs', 'Duty'], how='inner')\n",
    "    merged_test = pd.merge(s11_test, TDR_test, on=['Lg', 'Vds', 'Vgs', 'Duty'], how='inner')\n",
    "    merged_val = pd.merge(s11_val, TDR_val, on=['Lg', 'Vds', 'Vgs', 'Duty'], how='inner')\n",
    "    \n",
    "    # Separate the DataFrames back\n",
    "    s11_train = merged_train[s11_train.columns]\n",
    "    TDR_train = merged_train[TDR_train.columns]\n",
    "    \n",
    "    s11_test = merged_test[s11_test.columns]\n",
    "    TDR_test = merged_test[TDR_test.columns]\n",
    "    \n",
    "    s11_val = merged_val[s11_val.columns]\n",
    "    TDR_val = merged_val[TDR_val.columns]\n",
    "    \n",
    "    TDR_train = smooth_dataframe_columns(TDR_train, 63, 3, 1)\n",
    "    TDR_test = smooth_dataframe_columns(TDR_test, 63, 3, 1)\n",
    "    TDR_val = smooth_dataframe_columns(TDR_val, 63, 3, 1)\n",
    "    \n",
    "    X_s11_train = s11_train.iloc[:, 0:-1].values\n",
    "    y_s11_train = s11_train['Duty'].values\n",
    "\n",
    "    X_s11_test = s11_test.iloc[:, 0:-1].values\n",
    "    y_s11_test = s11_test['Duty'].values\n",
    "\n",
    "    X_s11_val = s11_val.iloc[:, 0:-1].values\n",
    "    y_s11_val = s11_val['Duty'].values\n",
    "\n",
    "    X_TDR_train = TDR_train.iloc[:, 0:-1].values\n",
    "    y_TDR_train = TDR_train['Duty'].values\n",
    "\n",
    "    X_TDR_test = TDR_test.iloc[:, 0:-1].values\n",
    "    y_TDR_test = TDR_test['Duty'].values\n",
    "\n",
    "    X_TDR_val = TDR_val.iloc[:, 0:-1].values\n",
    "    y_TDR_val = TDR_val['Duty'].values\n",
    "    \n",
    "    # Mapping of scaling types to scaler objects\n",
    "    scalers = {\n",
    "        'standard': StandardScaler(),\n",
    "        'minmax': MinMaxScaler(),\n",
    "        'robust': RobustScaler(),\n",
    "        'maxabs': MaxAbsScaler()\n",
    "    }\n",
    "\n",
    "    # Check if the scaling_type is valid\n",
    "    if scaling_type not in scalers:\n",
    "        raise ValueError(f\"Invalid scaling_type. Choose from {list(scalers.keys())}.\")\n",
    "    \n",
    "    # Select the appropriate scaler\n",
    "    scaler = scalers[scaling_type] \n",
    "    \n",
    "    X_s11_train_scaled = scaler.fit_transform(X_s11_train)\n",
    "    X_s11_test_scaled = scaler.transform(X_s11_test)\n",
    "    X_s11_val_scaled = scaler.transform(X_s11_val)\n",
    "        \n",
    "    X_TDR_train_scaled = scaler.fit_transform(X_TDR_train)\n",
    "    X_TDR_test_scaled = scaler.transform(X_TDR_test)\n",
    "    X_TDR_val_scaled = scaler.transform(X_TDR_val)\n",
    "    \n",
    "    # Convert data to PyTorch tensors\n",
    "    X_s11_train_scaled = torch.tensor(X_s11_train_scaled, dtype=torch.float32)\n",
    "    y_s11_train = torch.tensor(y_s11_train, dtype=torch.long)\n",
    "    X_s11_test_scaled = torch.tensor(X_s11_test_scaled, dtype=torch.float32)\n",
    "    y_s11_test = torch.tensor(y_s11_test, dtype=torch.long)\n",
    "    X_s11_val_scaled = torch.tensor(X_s11_val_scaled, dtype=torch.float32)\n",
    "    y_s11_val = torch.tensor(y_s11_val, dtype=torch.long)\n",
    "\n",
    "    # Convert data and labels to TensorDatasets and create DataLoaders\n",
    "    s11_train_dataset = TensorDataset(X_s11_train_scaled, y_s11_train.long())\n",
    "    s11_val_dataset = TensorDataset(X_s11_val_scaled, y_s11_val.long())\n",
    "    s11_test_dataset = TensorDataset(X_s11_test_scaled, y_s11_test.long())\n",
    "\n",
    "    s11_train_loader = torch.utils.data.DataLoader(s11_train_dataset, batch_size=4, shuffle=True)\n",
    "    s11_test_loader = torch.utils.data.DataLoader(s11_test_dataset, batch_size=4)\n",
    "    s11_val_loader = torch.utils.data.DataLoader(s11_val_dataset, batch_size=4)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_TDR_train_scaled = torch.tensor(X_TDR_train_scaled, dtype=torch.float32)\n",
    "    y_TDR_train = torch.tensor(y_TDR_train, dtype=torch.long)\n",
    "    X_TDR_test_scaled = torch.tensor(X_TDR_test_scaled, dtype=torch.float32)\n",
    "    y_TDR_test = torch.tensor(y_TDR_test, dtype=torch.long)\n",
    "    X_TDR_val_scaled = torch.tensor(X_TDR_val_scaled, dtype=torch.float32)\n",
    "    y_TDR_val = torch.tensor(y_TDR_val, dtype=torch.long)\n",
    "\n",
    "    # Convert data and labels to TensorDatasets and create DataLoaders\n",
    "    TDR_train_dataset = TensorDataset(X_TDR_train_scaled, y_TDR_train.long())\n",
    "    TDR_val_dataset = TensorDataset(X_TDR_val_scaled, y_TDR_val.long())\n",
    "    TDR_test_dataset = TensorDataset(X_TDR_test_scaled, y_TDR_test.long())\n",
    "\n",
    "    TDR_train_loader = torch.utils.data.DataLoader(TDR_train_dataset, batch_size=4, shuffle=True)\n",
    "    TDR_test_loader = torch.utils.data.DataLoader(TDR_test_dataset, batch_size=4)\n",
    "    TDR_val_loader = torch.utils.data.DataLoader(TDR_val_dataset, batch_size=4)\n",
    "    \n",
    "    return s11_train_loader, s11_test_loader, s11_val_loader, TDR_train_loader, TDR_test_loader, TDR_val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d35f33",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6319c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Model for Duty Classification\n",
    "\n",
    "class DutyClassifier(nn.Module):\n",
    "    def __init__(self, seq_len1, seq_len2, input_dim, num_classes, d_model=128, nhead=8, num_layers=1):\n",
    "        super(DutyClassifier, self).__init__()\n",
    "        \n",
    "        # Separate embedding layers to map input dimensions to d_model\n",
    "        self.embedding1 = nn.Linear(input_dim, d_model)\n",
    "        self.embedding2 = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Separate transformer encoders for each input\n",
    "        encoder_layer1 = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        encoder_layer2 = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        \n",
    "        self.transformer_encoder1 = nn.TransformerEncoder(encoder_layer1, num_layers=num_layers)\n",
    "        self.transformer_encoder2 = nn.TransformerEncoder(encoder_layer2, num_layers=num_layers)\n",
    "        \n",
    "        # Classification layer\n",
    "        # The total dimension is (seq_len1 + seq_len2) * d_model after concatenation\n",
    "\n",
    "        self.classifier = nn.Linear(d_model * (seq_len1 + seq_len2), num_classes)\n",
    "     \n",
    "    def forward(self, x1, x2):\n",
    "        # Project each dataset to the common feature dimension (d_model)\n",
    "        x1 = self.embedding1(x1)  # Shape: [batch_size, seq_len1, d_model]\n",
    "        x2 = self.embedding2(x2)  # Shape: [batch_size, seq_len2, d_model]\n",
    "        # Transform the sequence for each dataset\n",
    "        x1 = x1.permute(1, 0, 2)  # Shape: [seq_len1, batch_size, d_model]\n",
    "        x1 = self.transformer_encoder1(x1)\n",
    "        x1 = x1.permute(1, 0, 2)  # Back to [batch_size, seq_len1, d_model]\n",
    "\n",
    "        x2 = x2.permute(1, 0, 2)  # Shape: [seq_len2, batch_size, d_model]\n",
    "        x2 = self.transformer_encoder2(x2)\n",
    "        x2 = x2.permute(1, 0, 2)  # Back to [batch_size, seq_len2, d_model]\n",
    "\n",
    "        # Concatenate along the sequence length dimension\n",
    "        x = torch.cat((x1, x2), dim=1)  # Shape: [batch_size, seq_len1 + seq_len2, d_model]\n",
    "\n",
    "        # Flatten for classification\n",
    "        x = x.flatten(start_dim=1)  # Shape: [batch_size, (seq_len1 + seq_len2) * d_model]\n",
    "        out = self.classifier(x)  # Shape: [batch_size, num_classes]\n",
    "\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceb240e",
   "metadata": {},
   "source": [
    "## Training and testing of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52c3dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, num_epochs, patience):\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_targets, train_preds = [], []\n",
    "        \n",
    "        # Iterate over both loaders simultaneously (synchronized)\n",
    "        for (s11_train_data, train_labels), (tdr_train_data, _) in zip(s11_train_loader, TDR_train_loader):\n",
    "            # Move inputs and labels to the appropriate device\n",
    "            s11_train_data = s11_train_data.to(device)\n",
    "            tdr_train_data = tdr_train_data.to(device)\n",
    "            train_labels = train_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(s11_train_data.unsqueeze(-1), tdr_train_data.unsqueeze(-1))\n",
    "            loss = criterion(outputs, train_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            train_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            train_targets.extend(train_labels.cpu().numpy())\n",
    "                \n",
    "        train_accuracy = accuracy_score(train_targets, train_preds)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds, val_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for (s11_val_data, val_labels), (tdr_val_data, _) in zip(s11_val_loader, TDR_val_loader):\n",
    "                # Move inputs and labels to the appropriate device\n",
    "                s11_val_data = s11_val_data.to(device)\n",
    "                tdr_val_data = tdr_val_data.to(device)\n",
    "                val_labels = val_labels.to(device)\n",
    "                \n",
    "                \n",
    "                outputs = model(s11_val_data.unsqueeze(-1), tdr_val_data.unsqueeze(-1))\n",
    "                loss = criterion(outputs, val_labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                val_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "                val_targets.extend(val_labels.cpu().numpy())\n",
    "\n",
    "        val_accuracy = accuracy_score(val_targets, val_preds)\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs. Best Val Loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "            \n",
    "# Testing function\n",
    "def test_model(model):\n",
    "    model.eval()\n",
    "    test_preds, test_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (s11_test_data, test_labels), (tdr_test_data, _) in zip(s11_test_loader, TDR_test_loader):\n",
    "            # Move inputs and labels to the appropriate device\n",
    "            s11_test_data = s11_test_data.to(device)\n",
    "            tdr_test_data = tdr_test_data.to(device)\n",
    "            test_labels = test_labels.to(device)\n",
    "            \n",
    "            outputs = model(s11_test_data.unsqueeze(-1), tdr_test_data.unsqueeze(-1))\n",
    "            test_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            test_targets.extend(test_labels.cpu().numpy())\n",
    "\n",
    "    test_accuracy = accuracy_score(test_targets, test_preds)\n",
    "    test_conf_matrix = confusion_matrix(test_targets, test_preds)\n",
    "    classi_report = classification_report(test_targets, test_preds)\n",
    "    print(test_conf_matrix)\n",
    "    print('Confusion Matrix')\n",
    "    print(confusion_matrix)\n",
    "    print('Accuracy - ',np.round(test_accuracy,3))\n",
    "    print('Classification Report')\n",
    "    print(classi_report)\n",
    "    \n",
    "    return test_accuracy\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e438dd26-2593-4837-8b82-1e3f072066bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_9692\\4202307927.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  Pretrained_Multimodal_classifier.load_state_dict(torch.load(r'C:\\Master_thesis\\creating_dataset\\Model\\Multimodal_classifier_2.pth'))\n"
     ]
    }
   ],
   "source": [
    " # Initialize the model, loss function, and optimizer\n",
    "Pretrained_Multimodal_classifier = DutyClassifier(seq_len1=1003, seq_len2=1005, input_dim=1, num_classes=2)\n",
    "Pretrained_Multimodal_classifier.load_state_dict(torch.load(r'C:\\Master_thesis\\creating_dataset\\Model\\Multimodal_classifier_2.pth'))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Pretrained_Multimodal_classifier.parameters(), lr=0.0001)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Pretrained_Multimodal_classifier = Pretrained_Multimodal_classifier.to(device)\n",
    "\n",
    "for param in Pretrained_Multimodal_classifier.parameters():  # Freeze all parameters\n",
    "    param.requires_grad = False\n",
    "for param in Pretrained_Multimodal_classifier.classifier.parameters():  # Freeze all parameters\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7184d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "1th Training Loop\n",
      "Epoch 11/50, Train Accuracy: 0.9444, Val Accuracy: 0.5000\n",
      "Early stopping triggered after 11 epochs. Best Val Loss: 3.3702\n",
      "[[2 1]\n",
      " [1 2]]\n",
      "Confusion Matrix\n",
      "<function confusion_matrix at 0x000002059607F1A0>\n",
      "Accuracy -  0.667\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67         3\n",
      "           1       0.67      0.67      0.67         3\n",
      "\n",
      "    accuracy                           0.67         6\n",
      "   macro avg       0.67      0.67      0.67         6\n",
      "weighted avg       0.67      0.67      0.67         6\n",
      "\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "2th Training Loop\n",
      "Epoch 17/50, Train Accuracy: 1.0000, Val Accuracy: 0.5000\n",
      "Early stopping triggered after 17 epochs. Best Val Loss: 0.6453\n",
      "[[2 1]\n",
      " [1 2]]\n",
      "Confusion Matrix\n",
      "<function confusion_matrix at 0x000002059607F1A0>\n",
      "Accuracy -  0.667\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67         3\n",
      "           1       0.67      0.67      0.67         3\n",
      "\n",
      "    accuracy                           0.67         6\n",
      "   macro avg       0.67      0.67      0.67         6\n",
      "weighted avg       0.67      0.67      0.67         6\n",
      "\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "3th Training Loop\n",
      "[[3 0]\n",
      " [1 2]]\n",
      "Confusion Matrix\n",
      "<function confusion_matrix at 0x000002059607F1A0>\n",
      "Accuracy -  0.833\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         3\n",
      "           1       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.83         6\n",
      "   macro avg       0.88      0.83      0.83         6\n",
      "weighted avg       0.88      0.83      0.83         6\n",
      "\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "4th Training Loop\n",
      "Epoch 13/50, Train Accuracy: 1.0000, Val Accuracy: 0.5000\n",
      "Early stopping triggered after 13 epochs. Best Val Loss: 4.8954\n",
      "[[3 0]\n",
      " [1 2]]\n",
      "Confusion Matrix\n",
      "<function confusion_matrix at 0x000002059607F1A0>\n",
      "Accuracy -  0.833\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         3\n",
      "           1       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.83         6\n",
      "   macro avg       0.88      0.83      0.83         6\n",
      "weighted avg       0.88      0.83      0.83         6\n",
      "\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "5th Training Loop\n",
      "Epoch 45/50, Train Accuracy: 1.0000, Val Accuracy: 0.5000\n",
      "Early stopping triggered after 45 epochs. Best Val Loss: 4.2614\n",
      "[[3 0]\n",
      " [1 2]]\n",
      "Confusion Matrix\n",
      "<function confusion_matrix at 0x000002059607F1A0>\n",
      "Accuracy -  0.833\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         3\n",
      "           1       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.83         6\n",
      "   macro avg       0.88      0.83      0.83         6\n",
      "weighted avg       0.88      0.83      0.83         6\n",
      "\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "6th Training Loop\n",
      "Epoch 29/50, Train Accuracy: 1.0000, Val Accuracy: 0.5000\n",
      "Early stopping triggered after 29 epochs. Best Val Loss: 4.1578\n",
      "[[3 0]\n",
      " [1 2]]\n",
      "Confusion Matrix\n",
      "<function confusion_matrix at 0x000002059607F1A0>\n",
      "Accuracy -  0.833\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         3\n",
      "           1       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.83         6\n",
      "   macro avg       0.88      0.83      0.83         6\n",
      "weighted avg       0.88      0.83      0.83         6\n",
      "\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "7th Training Loop\n",
      "Epoch 16/50, Train Accuracy: 1.0000, Val Accuracy: 0.5000\n",
      "Early stopping triggered after 16 epochs. Best Val Loss: 4.0792\n",
      "[[3 0]\n",
      " [1 2]]\n",
      "Confusion Matrix\n",
      "<function confusion_matrix at 0x000002059607F1A0>\n",
      "Accuracy -  0.833\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         3\n",
      "           1       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.83         6\n",
      "   macro avg       0.88      0.83      0.83         6\n",
      "weighted avg       0.88      0.83      0.83         6\n",
      "\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "8th Training Loop\n",
      "Epoch 40/50, Train Accuracy: 1.0000, Val Accuracy: 0.5000\n",
      "Early stopping triggered after 40 epochs. Best Val Loss: 3.8539\n",
      "[[3 0]\n",
      " [1 2]]\n",
      "Confusion Matrix\n",
      "<function confusion_matrix at 0x000002059607F1A0>\n",
      "Accuracy -  0.833\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         3\n",
      "           1       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.83         6\n",
      "   macro avg       0.88      0.83      0.83         6\n",
      "weighted avg       0.88      0.83      0.83         6\n",
      "\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "9th Training Loop\n",
      "Epoch 13/50, Train Accuracy: 1.0000, Val Accuracy: 0.5000\n",
      "Early stopping triggered after 13 epochs. Best Val Loss: 3.8603\n",
      "[[3 0]\n",
      " [1 2]]\n",
      "Confusion Matrix\n",
      "<function confusion_matrix at 0x000002059607F1A0>\n",
      "Accuracy -  0.833\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         3\n",
      "           1       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.83         6\n",
      "   macro avg       0.88      0.83      0.83         6\n",
      "weighted avg       0.88      0.83      0.83         6\n",
      "\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "10th Training Loop\n",
      "Epoch 23/50, Train Accuracy: 1.0000, Val Accuracy: 0.5000\n",
      "Early stopping triggered after 23 epochs. Best Val Loss: 3.8110\n",
      "[[3 0]\n",
      " [1 2]]\n",
      "Confusion Matrix\n",
      "<function confusion_matrix at 0x000002059607F1A0>\n",
      "Accuracy -  0.833\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         3\n",
      "           1       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.83         6\n",
      "   macro avg       0.88      0.83      0.83         6\n",
      "weighted avg       0.88      0.83      0.83         6\n",
      "\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = []\n",
    "best_accuracy = 0\n",
    "for i in range(1, 11):\n",
    "    \n",
    "    print('-------------------------------------------------')\n",
    "    print(f'{i}th Training Loop')\n",
    "    \n",
    "    s11_train_loader, s11_test_loader, s11_val_loader, TDR_train_loader, TDR_test_loader, TDR_val_loader = data_Scaling_Sync(sync_s11_dataset, sync_TDR_dataset, scaling_type='standard')\n",
    "    train_model(Pretrained_Multimodal_classifier, num_epochs=50, patience=10)\n",
    "    test_preds = test_model(Pretrained_Multimodal_classifier)\n",
    "    test_accuracy.append(test_preds)\n",
    "    \n",
    "    if test_preds > best_accuracy:\n",
    "        best_accuracy = test_preds\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    print('-------------------------------------------------')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3714518d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.8333333333333334,\n",
       " 0.8333333333333334,\n",
       " 0.8333333333333334,\n",
       " 0.8333333333333334,\n",
       " 0.8333333333333334,\n",
       " 0.8333333333333334,\n",
       " 0.8333333333333334,\n",
       " 0.8333333333333334]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3665368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafe1ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5ba766-cb2d-4083-8277-75317a1c9143",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
